predicted_eval[4,4] <- 'top vs low'
predicted_eval
a
predicted_eval[5,4] <- 'low vs mid'
predicted_eval[6,4] <- 'mid vs mid'
predicted_eval[7,4] <- 'high vs mid'
predicted_eval[8,4] <- 'top vs mid'
predicted_eval[9,4] <- 'low vs high'
predicted_eval[10,4] <- 'mid vs high'
predicted_eval[11,4] <- 'high vs high'
predicted_eval[12,4] <- 'top vs high'
predicted_eval[1,4] <- 'low vs low'
predicted_eval[2,4] <- 'mid vs low'
predicted_eval[3,4] <- 'high vs low'
predicted_eval[4,4] <- 'top vs low'
predicted_eval[5,4] <- 'low vs mid'
predicted_eval[6,4] <- 'mid vs mid'
predicted_eval[7,4] <- 'high vs mid'
predicted_eval[8,4] <- 'top vs mid'
predicted_eval[9,4] <- 'low vs high'
predicted_eval[10,4] <- 'mid vs high'
predicted_eval[11,4] <- 'high vs high'
predicted_eval[12,4] <- 'top vs high'
predicted_eval[13,4] <- 'low vs top'
predicted_eval[14,4] <- 'mid vs top'
predicted_eval[15,4] <- 'high vs top'
predicted_eval[16,4] <- 'top vs top'
predicted_eval
write.csv(predicted_eval,"data/accuracy_of_prediction_by_rank.csv")
library(ggplot2)
ggplot(predicted_eval,aes(x=finalyrank,y=completly accurate prediction))+geom_point(size=4)
names(predicted_eval)<-names("completly_accurate_prediction","incomplete_prediction","completly_false_prediction","finaly_rank")
names(predicted_eval)<-c("completly_accurate_prediction","incomplete_prediction","completly_false_prediction","finaly_rank")
ggplot(predicted_eval,aes(x=finaly_rank,y=completly_accurate_prediction))+geom_point(size=4)
predicted_eval$completly_accurate_prediction <- factor(predicted_eval$finaly_rank,levels = predicted_eval$finaly_rank[order(predicted_eval$completly_accurate_prediction)])
ggplot(predicted_eval,aes(x=finaly_rank,y=completly_accurate_prediction))+geom_point(size=4)
predicted_eval$completly_accurate_prediction <- factor(predicted_eval$finaly_rank,levels = predicted_eval$finaly_rank[order(predicted_eval$completly_accurate_prediction)])
predicted_eval
j = 1
a[64]
predicted_eval <- data.frame()
for(i in 1:16){
FF <- a[j]
j = j+1
TF <- a[j]+a[j+1]
j = j+2
TT <- a[j]
j = j+1
completly_predicted <- TT/(TT+TF+FF)
not_completly_predicted <- TF/(TT+TF+FF)
false_predicted <- FF/(TT+TF+FF)
predicted_eval <- rbind(predicted_eval,c(completly_predicted,not_completly_predicted,false_predicted))
}
names(predicted_eval)<-c("completly_accurate_prediction","incomplete_prediction","completly_false_prediction")
predicted_eval
a
predicted_eval$finalyrank <- NA
predicted_eval[1,4] <- 'low vs low'
predicted_eval[2,4] <- 'mid vs low'
predicted_eval[3,4] <- 'high vs low'
predicted_eval[4,4] <- 'top vs low'
predicted_eval[5,4] <- 'low vs mid'
predicted_eval[6,4] <- 'mid vs mid'
predicted_eval[7,4] <- 'high vs mid'
predicted_eval[8,4] <- 'top vs mid'
predicted_eval[9,4] <- 'low vs high'
predicted_eval[10,4] <- 'mid vs high'
predicted_eval[11,4] <- 'high vs high'
predicted_eval[12,4] <- 'top vs high'
predicted_eval[13,4] <- 'low vs top'
predicted_eval[14,4] <- 'mid vs top'
predicted_eval[15,4] <- 'high vs top'
predicted_eval[16,4] <- 'top vs top'
predicted_eval
names(predicted_eval)<-c("completly_accurate_prediction","incomplete_prediction","completly_false_prediction","finaly_rank")
predicted_eval$finaly_rank <- factor(predicted_eval$finaly_rank,levels = predicted_eval$finaly_rank[order(predicted_eval$completly_accurate_prediction)])
ggplot(predicted_eval,aes(x=finaly_rank,y=completly_accurate_prediction))+geom_point(size=4)
summary(df)
write.csv(df,"data/df_with_rating.csv")
df <- read.csv("data/df_with_rating.csv")
summary(df)
summary(df)
df_new <- df[,c(41,42,36,33,10,13,16,17,18,19,20,21,22,23,24,25,26,27,35)]
summary(df_new)
df_new <- df[,c(40,41,35,32,10,13,15,16,17,18,19,20,21,22,23,24,25,26,34)]
summary(df_new)
ncol(df_new)
#classification tree
install.packages("caret")
install.packages("caret")
library(caret)
library(car)
install.packages("car")
library(car)
train.indices <- createDataPartition(df_new$more_2.5,p=0.8)
install.packages("e1071")
library(e1071)
train.indices <- createDataPartition(df_new$more_2.5,p=0.8)
#classification tree
install.packages("caret")
#classification tree
install.packages("caret")
library(caret)
library(caret)
library(caret)
library(caret)
install.packages("rlang")
library(caret)
install.packages("rlang")
install.packages("rlang")
#classification tree
install.packages("caret")
library(caret)
install.packages("vctrs")
library(caret)
train.indices <- createDataPartition(df_new$more_2.5,p=0.8)
summary(df_new[train.indices])
summary(df_new[train.indices,])
train.indices
train.df <- df_new[train.indices,]
test.df <-df_new[-train.indices,]
ncol(df_new)
summary(df_new)
train.indices <- createDataPartition(df_new$more_2.5,p=0.8,list=FALSE)
train.df <- df_new[train.indices,]
test.df <-df_new[-train.indices,]
summary(train.df)
summary(test.df)
install.packages("rpart")
install.packages("rpart")
library(e1071)
library(rpart)
ncol(train.df)
dt.cv <- train(x=train.df[,-19],
y=train.df$more_2.5,
method='rpart',
control=rpart.control(minsplit=10),
trControl=numFolds,
tuneGrid=cpGrid)
library(e1071)
dt.cv <- train(x=train.df[,-19],
y=train.df$more_2.5,
method='rpart',
control=rpart.control(minsplit=10),
trControl=numFolds,
tuneGrid=cpGrid)
t <- rpart(more_2.5~.,data = train.df)
t
dt.cv <- train(x=train.df[,-19],
y=train.df$more_2.5,
method='rpart',
control=rpart.control(minsplit=10),
trControl=numFolds,
tuneGrid=cpGrid)
#classification tree
install.packages("caret")
library(caret)
dt.cv <- train(x=train.df[,-19],
y=train.df$more_2.5,
method='rpart',
control=rpart.control(minsplit=10),
trControl=numFolds,
tuneGrid=cpGrid)
numFolds <- trainControl(method='cv',number=10)
cpGrid = expand.grid(.cp=seq(0.001,0.05,0.0025))
set.seed(10)
dt.cv <- train(x=train.df[,-19],
y=train.df$more_2.5,
method='rpart',
control=rpart.control(minsplit=10),
trControl=numFolds,
tuneGrid=cpGrid)
optimal_cp <- dt.cv$bestTune$cp
optimal_cp
tree1 <- rpart(more_2.5~.,data=train.df,method='class',
control=rpart.control(minsplit=10,cp=optimal_cp))
tree1
install.packages("rpart.plot")
library(rpart.plot)
rpart.plot(tree1)
print(tree1)
print(tree1)
tree1.pred <- predict(tree1,newdata = test.df,type='class')
tree1.pred
tree1.cm <- table(true=test.df$more_2.5,predicted=tree1.pred)
tree1.cm
tree1.accuracy <- (sum(diag(tree1.cm))/sum(tree1.cm))*100
tree1.accuracy
install.packages("randomForest")
library(randomForest)
results_df <- data.frame(matrix(ncol = 8))
colnames(results_df)[1]="No. of trees"
colnames(results_df)[2]="No. of variables"
colnames(results_df)[3]="Dev_AUC"
colnames(results_df)[4]="Dev_Hit_rate"
colnames(results_df)[5]="Dev_Coverage_rate"
colnames(results_df)[6]="Val_AUC"
colnames(results_df)[7]="Val_Hit_rate"
colnames(results_df)[8]="Val_Coverage_rate"
trees = c(50,100,150,250)
variables = c(8,10,15,20)
for(i in 1:length(trees))
{
ntree = trees[i]
for(j in 1:length(variables))
{
mtry = variables[j]
rf<-randomForest(train.df[,-19],train.df$more_2.5,ntree=ntree,mtry=mtry)
pred<-as.data.frame(predict(rf,type="class"))
class_rf<-cbind(dev$Target,pred)
colnames(class_rf)[1]<-"actual_values"
colnames(class_rf)[2]<-"predicted_values"
dev_hit_rate = nrow(subset(class_rf, actual_values ==1&predicted_values==1))/nrow(subset(class_rf, predicted_values ==1))
dev_coverage_rate = nrow(subset(class_rf, actual_values ==1&predicted_values==1))/nrow(subset(class_rf, actual_values ==1))
pred_prob<-as.data.frame(predict(rf,type="prob"))
prob_rf<-cbind(dev$Target,pred_prob)
colnames(prob_rf)[1]<-"target"
colnames(prob_rf)[2]<-"prob_0"
colnames(prob_rf)[3]<-"prob_1"
pred<-prediction(prob_rf$prob_1,prob_rf$target)
auc <- performance(pred,"auc")
dev_auc<-as.numeric(auc@y.values)
pred<-as.data.frame(predict(rf,val,type="class"))
class_rf<-cbind(val$Target,pred)
colnames(class_rf)[1]<-"actual_values"
colnames(class_rf)[2]<-"predicted_values"
val_hit_rate = nrow(subset(class_rf, actual_values ==1&predicted_values==1))/nrow(subset(class_rf, predicted_values ==1))
val_coverage_rate = nrow(subset(class_rf, actual_values ==1&predicted_values==1))/nrow(subset(class_rf, actual_values ==1))
pred_prob<-as.data.frame(predict(rf,val,type="prob"))
prob_rf<-cbind(val$Target,pred_prob)
colnames(prob_rf)[1]<-"target"
colnames(prob_rf)[2]<-"prob_0"
colnames(prob_rf)[3]<-"prob_1"
pred<-prediction(prob_rf$prob_1,prob_rf$target)
auc <- performance(pred,"auc")
val_auc<-as.numeric(auc@y.values)
results_df = rbind(results_df,c(ntree,mtry,dev_auc,dev_hit_rate,dev_coverage_rate,val_auc,val_hit_rate,val_coverage_rate))
}
}
results_df <- data.frame(matrix(ncol = 8))
colnames(results_df)[1]="No. of trees"
colnames(results_df)[2]="No. of variables"
colnames(results_df)[3]="Dev_AUC"
colnames(results_df)[4]="Dev_Hit_rate"
colnames(results_df)[5]="Dev_Coverage_rate"
colnames(results_df)[6]="Val_AUC"
colnames(results_df)[7]="Val_Hit_rate"
colnames(results_df)[8]="Val_Coverage_rate"
trees = c(50,100,150,250)
variables = c(8,10,15,20)
for(i in 1:length(trees))
{
ntree = trees[i]
for(j in 1:length(variables))
{
mtry = variables[j]
rf<-randomForest(train.df[,-19],train.df$more_2.5,ntree=ntree,mtry=mtry)
pred<-as.data.frame(predict(rf,type="class"))
class_rf<-cbind(train.df$more_2.5,pred)
colnames(class_rf)[1]<-"actual_values"
colnames(class_rf)[2]<-"predicted_values"
dev_hit_rate = nrow(subset(class_rf, actual_values ==1&predicted_values==1))/nrow(subset(class_rf, predicted_values ==1))
dev_coverage_rate = nrow(subset(class_rf, actual_values ==1&predicted_values==1))/nrow(subset(class_rf, actual_values ==1))
pred_prob<-as.data.frame(predict(rf,type="prob"))
prob_rf<-cbind(train.df$more_2.5,pred_prob)
colnames(prob_rf)[1]<-"target"
colnames(prob_rf)[2]<-"prob_0"
colnames(prob_rf)[3]<-"prob_1"
pred<-prediction(prob_rf$prob_1,prob_rf$target)
auc <- performance(pred,"auc")
dev_auc<-as.numeric(auc@y.values)
pred<-as.data.frame(predict(rf,val,type="class"))
class_rf<-cbind(test.df$more_2.5,pred)
colnames(class_rf)[1]<-"actual_values"
colnames(class_rf)[2]<-"predicted_values"
val_hit_rate = nrow(subset(class_rf, actual_values ==1&predicted_values==1))/nrow(subset(class_rf, predicted_values ==1))
val_coverage_rate = nrow(subset(class_rf, actual_values ==1&predicted_values==1))/nrow(subset(class_rf, actual_values ==1))
pred_prob<-as.data.frame(predict(rf,val,type="prob"))
prob_rf<-cbind(test.df$more_2.5,pred_prob)
colnames(prob_rf)[1]<-"target"
colnames(prob_rf)[2]<-"prob_0"
colnames(prob_rf)[3]<-"prob_1"
pred<-prediction(prob_rf$prob_1,prob_rf$target)
auc <- performance(pred,"auc")
val_auc<-as.numeric(auc@y.values)
results_df = rbind(results_df,c(ntree,mtry,dev_auc,dev_hit_rate,dev_coverage_rate,val_auc,val_hit_rate,val_coverage_rate))
}
}
install.packages("ROCR")
library(ROCR)
results_df <- data.frame(matrix(ncol = 8))
colnames(results_df)[1]="No. of trees"
colnames(results_df)[2]="No. of variables"
colnames(results_df)[3]="Dev_AUC"
colnames(results_df)[4]="Dev_Hit_rate"
colnames(results_df)[5]="Dev_Coverage_rate"
colnames(results_df)[6]="Val_AUC"
colnames(results_df)[7]="Val_Hit_rate"
colnames(results_df)[8]="Val_Coverage_rate"
trees = c(50,100,150,250)
variables = c(8,10,15,20)
for(i in 1:length(trees))
{
ntree = trees[i]
for(j in 1:length(variables))
{
mtry = variables[j]
rf<-randomForest(train.df[,-19],train.df$more_2.5,ntree=ntree,mtry=mtry)
pred<-as.data.frame(predict(rf,type="class"))
class_rf<-cbind(train.df$more_2.5,pred)
colnames(class_rf)[1]<-"actual_values"
colnames(class_rf)[2]<-"predicted_values"
dev_hit_rate = nrow(subset(class_rf, actual_values ==1&predicted_values==1))/nrow(subset(class_rf, predicted_values ==1))
dev_coverage_rate = nrow(subset(class_rf, actual_values ==1&predicted_values==1))/nrow(subset(class_rf, actual_values ==1))
pred_prob<-as.data.frame(predict(rf,type="prob"))
prob_rf<-cbind(train.df$more_2.5,pred_prob)
colnames(prob_rf)[1]<-"target"
colnames(prob_rf)[2]<-"prob_0"
colnames(prob_rf)[3]<-"prob_1"
pred<-prediction(prob_rf$prob_1,prob_rf$target)
auc <- performance(pred,"auc")
dev_auc<-as.numeric(auc@y.values)
pred<-as.data.frame(predict(rf,val,type="class"))
class_rf<-cbind(test.df$more_2.5,pred)
colnames(class_rf)[1]<-"actual_values"
colnames(class_rf)[2]<-"predicted_values"
val_hit_rate = nrow(subset(class_rf, actual_values ==1&predicted_values==1))/nrow(subset(class_rf, predicted_values ==1))
val_coverage_rate = nrow(subset(class_rf, actual_values ==1&predicted_values==1))/nrow(subset(class_rf, actual_values ==1))
pred_prob<-as.data.frame(predict(rf,val,type="prob"))
prob_rf<-cbind(test.df$more_2.5,pred_prob)
colnames(prob_rf)[1]<-"target"
colnames(prob_rf)[2]<-"prob_0"
colnames(prob_rf)[3]<-"prob_1"
pred<-prediction(prob_rf$prob_1,prob_rf$target)
auc <- performance(pred,"auc")
val_auc<-as.numeric(auc@y.values)
results_df = rbind(results_df,c(ntree,mtry,dev_auc,dev_hit_rate,dev_coverage_rate,val_auc,val_hit_rate,val_coverage_rate))
}
}
for(i in 1:length(trees))
{
ntree = trees[i]
for(j in 1:length(variables))
{
mtry = variables[j]
rf<-randomForest(train.df[,-19],train.df$more_2.5,ntree=ntree,mtry=mtry)
pred<-as.data.frame(predict(rf,type="class"))
class_rf<-cbind(train.df$more_2.5,pred)
colnames(class_rf)[1]<-"actual_values"
colnames(class_rf)[2]<-"predicted_values"
dev_hit_rate = nrow(subset(class_rf, actual_values ==1&predicted_values==1))/nrow(subset(class_rf, predicted_values ==1))
dev_coverage_rate = nrow(subset(class_rf, actual_values ==1&predicted_values==1))/nrow(subset(class_rf, actual_values ==1))
pred_prob<-as.data.frame(predict(rf,type="prob"))
prob_rf<-cbind(train.df$more_2.5,pred_prob)
colnames(prob_rf)[1]<-"target"
colnames(prob_rf)[2]<-"prob_0"
colnames(prob_rf)[3]<-"prob_1"
pred<-prediction(prob_rf$prob_1,prob_rf$target)
auc <- performance(pred,"auc")
dev_auc<-as.numeric(auc@y.values)
pred<-as.data.frame(predict(rf,val,type="class"))
class_rf<-cbind(test.df$more_2.5,pred)
colnames(class_rf)[1]<-"actual_values"
colnames(class_rf)[2]<-"predicted_values"
val_hit_rate = nrow(subset(class_rf, actual_values ==1&predicted_values==1))/nrow(subset(class_rf, predicted_values ==1))
val_coverage_rate = nrow(subset(class_rf, actual_values ==1&predicted_values==1))/nrow(subset(class_rf, actual_values ==1))
pred_prob<-as.data.frame(predict(rf,test.df$more_2.5,type="prob"))
prob_rf<-cbind(test.df$more_2.5,pred_prob)
colnames(prob_rf)[1]<-"target"
colnames(prob_rf)[2]<-"prob_0"
colnames(prob_rf)[3]<-"prob_1"
pred<-prediction(prob_rf$prob_1,prob_rf$target)
auc <- performance(pred,"auc")
val_auc<-as.numeric(auc@y.values)
results_df = rbind(results_df,c(ntree,mtry,dev_auc,dev_hit_rate,dev_coverage_rate,val_auc,val_hit_rate,val_coverage_rate))
}
}
for(i in 1:length(trees))
{
ntree = trees[i]
for(j in 1:length(variables))
{
mtry = variables[j]
rf<-randomForest(train.df[,-19],train.df$more_2.5,ntree=ntree,mtry=mtry)
pred<-as.data.frame(predict(rf,type="class"))
class_rf<-cbind(train.df$more_2.5,pred)
colnames(class_rf)[1]<-"actual_values"
colnames(class_rf)[2]<-"predicted_values"
dev_hit_rate = nrow(subset(class_rf, actual_values ==1&predicted_values==1))/nrow(subset(class_rf, predicted_values ==1))
dev_coverage_rate = nrow(subset(class_rf, actual_values ==1&predicted_values==1))/nrow(subset(class_rf, actual_values ==1))
pred_prob<-as.data.frame(predict(rf,type="prob"))
prob_rf<-cbind(train.df$more_2.5,pred_prob)
colnames(prob_rf)[1]<-"target"
colnames(prob_rf)[2]<-"prob_0"
colnames(prob_rf)[3]<-"prob_1"
pred<-prediction(prob_rf$prob_1,prob_rf$target)
auc <- performance(pred,"auc")
dev_auc<-as.numeric(auc@y.values)
pred<-as.data.frame(predict(rf,test.df$more_2.5,type="class"))
class_rf<-cbind(test.df$more_2.5,pred)
colnames(class_rf)[1]<-"actual_values"
colnames(class_rf)[2]<-"predicted_values"
val_hit_rate = nrow(subset(class_rf, actual_values ==1&predicted_values==1))/nrow(subset(class_rf, predicted_values ==1))
val_coverage_rate = nrow(subset(class_rf, actual_values ==1&predicted_values==1))/nrow(subset(class_rf, actual_values ==1))
pred_prob<-as.data.frame(predict(rf,test.df$more_2.5,type="prob"))
prob_rf<-cbind(test.df$more_2.5,pred_prob)
colnames(prob_rf)[1]<-"target"
colnames(prob_rf)[2]<-"prob_0"
colnames(prob_rf)[3]<-"prob_1"
pred<-prediction(prob_rf$prob_1,prob_rf$target)
auc <- performance(pred,"auc")
val_auc<-as.numeric(auc@y.values)
results_df = rbind(results_df,c(ntree,mtry,dev_auc,dev_hit_rate,dev_coverage_rate,val_auc,val_hit_rate,val_coverage_rate))
}
}
model1 <- randomForest(more_2.5~.,data=train.df,ntree=500,mtry=6,importance=TRUE)
model1
rf1.pred <- predict(model1,test.df,type='class')
rf1.pred
rf1.cm <- table(predicted=rf1.pred,true=test.df$more_2.5)
rf1.cm
rf1.accuracy <- (sum(diag(rf1.cm))/sum(rf1.cm))*100
rf1.accuracy
tree1.accuracy
control <- trainControl(method='repeatedcv',number=10,repeats = 3,search = 'grid')
set.seed(190)
tuneGrid <- expand.grid(.mtry=c(1:15))
rf_gridsearch <- train(more_2.5~.,data=train.df,method='rf',tuneGrid=tuneGrid,trControl=control)
control <- trainControl(method='repeatedcv',number=10,repeats = 3,search = 'grid')
set.seed(190)
tuneGrid <- expand.grid(.mtry=c(1:15))
rf_gridsearch <- train(more_2.5~.,data=train.df,method='rf',tuneGrid=tuneGrid,trControl=control)
library(caret)
control <- trainControl(method='repeatedcv',number=10,repeats = 3,search = 'grid')
set.seed(190)
tuneGrid <- expand.grid(.mtry=c(1:15))
rf_gridsearch <- train(more_2.5~.,data=train.df,method='rf',tuneGrid=tuneGrid,trControl=control)
rf_gridsearch$bestTune$mtry
optimal_mtry <- rf_gridsearch$bestTune$mtry
plot(rf_gridsearch)
customRF <- list(type = "Classification", library = "randomForest", loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree"), class = rep("numeric", 2), label = c("mtry", "ntree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes
control <- trainControl(method="repeatedcv", number=10, repeats=3)
tunegrid <- expand.grid(.mtry=c(1:15), .ntree=c(1000, 1500, 2000, 2500))
set.seed(seed)
custom <- train(more_2.5~., data=train.df, method=customRF, tuneGrid=tunegrid, trControl=control)
summary(custom)
plot(custom)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
tunegrid <- expand.grid(.mtry=c(1:15), .ntree=c(1000, 1500, 2000, 2500))
set.seed(190)
custom <- train(more_2.5~., data=train.df, method=customRF, tuneGrid=tunegrid, trControl=control)
summary(custom)
plot(custom)
rf_mtry <- custom$bestTune$mtry
rf_ntree <- custom$bestTune$ntree
model2 <- randomForest(more_2.5~.,data=train.df,ntree=rf_ntree,mtry=rf_mtry,importance=TRUE)
model2
rf2.pred <- predict(model2,test.df,type='class')
rf2.pred
rf2.cm <- table(predicted=rf2.pred,true=test.df$more_2.5)
rf2.cm
rf2.accuracy <- (sum(diag(rf2.cm))/sum(rf2.cm))*100
rf2.accuracy
summary(df)
summary(df$true_predicted_goals)
summary(df$true_predicted_goals[[1]])
summary(df$true_predicted_goals[1])
summary(df$true_predicted_goals)
accuracy_by_bet <- (230/380)*100
accuracy_by_bet
install.package("ISLR")
install.packages("ISLR")
library(ISLR)
glm.fit <- glm(more_2.5~.,
data=train.df,
family=binomial)
summary(glm.fit)
glm.probs <- predict(glm.fit,
newdata=test.df,
type='response')
glm.pred <- ifelse(glm.probs>0.5,'Yes','No')
glm.pred
glm.cm <- table(predicted=glm.pred,true=test.df$more_2.5)
glm.cm
glm.accuracy <- (sum(diag(glm.cm))/sum(glm.cm))
glm.accuracy
summary(glm.fit)
summary(df)
glm.smaller <- glm(more_2.5~FTRD+HTRD+HST+AST+HR)
glm.smaller <- glm(more_2.5~FTRD+HTRD+HST+AST+HR,data=train.df,family = binomial)
summary(df)
summary(glm.fit)
glm.smaller <- glm(more_2.5~FTRD+HTRD+HST+AST+HR,data=train.df,family = binomial)
glm.smaller <- glm(more_2.5~HST+AST+HR,data=train.df,family = binomial)
summary(glm.smaller)
glm.smaller.probs <- predict(glm.smaller,newdata = test.df,type='response')
glm.smaller.pred <- ifelse(glm.smaller.probs>0.5,'Yes','No')
table(true=test.df$more_2.5,predicted=glm.smaller.pred)
glm.smaller.cm<-table(true=test.df$more_2.5,predicted=glm.smaller.pred)
glm.smaller.accuracy <- (sum(diag(glm.smaller.cm))/sum(glm.smaller.cm))
glm.smaller.accuracy
